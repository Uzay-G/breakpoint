# Breakpoint

**Breakpoint** is a benchmark and toolkit to evaluate the local and systems-level reasoning abilities of Large Language Models (LLMs) through adversarially generated code repair tasks.

With Breakpoint you can:

- create new SWE problems for language models on arbitrary Python/Pytest codebases
- select these SWE problems based on principled static difficulty properties like code complexity and centrality
- evaluate SWE agents on large datasets of these SWE problems whose difficulty smoothly scales beyond human level

Breakpoint generates tasks by introducing corruptions in real-world code repositories, and then explicitly testing a model's ability to diagnose, explore and repair the codebase. See the [paper](https://arxiv.org/pdf/2506.00172) for more information and additional motivation.

## Getting Started

Clone this repository:

```sh
git clone https://github.com/Uzay-G/breakpoint.git
cd breakpoint
```

Install dependencies:

```sh
pip install -r requirements.txt
```

Below we explain how to use Breakpoint to generate and test models. If you don't want to generate your own problems, you can simply download our data files from [here](https://huggingface.co/datasets/uzpg/breakpoint/tree/main/data).

## Generating Custom SWE Problems


There are 2 corruption modes to create Breakpoint problems. In `remove` mode we simply delete function bodies from the code, which the model then has to infer to solve the task.

In `discovery` mode we introduce a more subtle corruption of the code that is generated by another SWE agent, and the model has to discover where in the code it is to then fix it.

You can generate both of these Breakpoint task modes on arbitrary code, if it's a Python repository with pytest testing.

The central data generation script is run with `python3 -m lib.problem_generator --dump_file <path> <command> options/arguments`

### Process repository for candidate functions

First Breakpoint processes your code to find suitable functions to corrupt.

For example:

```
python3 -m lib.problem_generator --dump_file archivy_problems.json cache --num_workers 20  --num_functions 15 --code_path archivy --repo_path ~/repos/archivy
```

This would source 15 functions from the Archivy repository stored locally. The `code_path` points to the location of the source code within the repository. This generates a list of functions that were deemed good as corruptions. 
See `python3 -m lib.problem_generator cacheall --help` for options on sourcing from a whole directory of codebases.

### Generating Corruptions

You can directly use the file from the last step to run `remove` mode evaluations. You can also use it to generate `discovery` mode problems.

The command to source language model generated corruptions is the following:

```
python3 -m lib.problem_generator --dump_file corruptions.json corruptall --cache_path <file from previous step> --num_corruptions NUM_CORRUPTIONS --model MODEL --num_workers NUM_WORKERS
```

This will save language model corruptions in `corruptions.json`.


## Running Evaluations

Evaluate your model on Breakpoint using the `runners/eval.py` script. Run the command as `python3 -m runners.eval --option1 <option val>` etc...

The main arguments and options are:

Run code benchmark evaluation

Options:

```
  --data DATA           Path to the evaluation data json file
  --workers WORKERS     Number of concurrent workers
  --output OUTPUT       Output file for results
  --max_iterations MAX_ITERATIONS   Max agent tool iterations
  --test_budget TEST_BUDGET  how many tests the agent can call
  --mode MODE remove or discovery
  --thinking_budget THINKING_BUDGET how many tokens the model can think for
  --model MODEL [MODEL ...] Model name(s) to evaluate. Can provide multiple models
  --n_problems N_PROBLEMS
  --multi MULTI         For discovery, apply multiple corruptions at once
```

For example:

```
python3 -m runners.eval --model o4-mini --data <data file from previous step> --mode remove  --workers 15 --max_iterations 16 --test_budget 4 --n_problems 10 --output o4-run.json
```

This will produce a JSONL with detailed agent traces and the perfect solve rate of the model which you can then analyze using our consolidator and plotter scripts.

If you have any problems along the way please open an issue!

## Contributing

We welcome contributions to improve Breakpoint. Please submit issues or pull requests through GitHub, or contact us `fulcrum[at]mit.edu` if you have questions or comments.

## Citing

If you use Breakpoint in your research, please cite:

TODO insert citation

If you find it useful, let us know at fulcrum[at]mit.edu! Would love to hear more about your usecase and results.


## Additional Features

### Human Evaluation

You can run the benchmark with human participants by specifying `--model human`. This will use the same infrastructure as the automated models, allowing you to compare human performance directly against AI models.

When you run with the `--model human` flag, the system will:
1. Set up the problem environment with the corrupted/broken function
2. Create a `HUMAN_EVAL_README.md` file with detailed instructions
3. Print the location of the problem directory
4. Exit, allowing the human to work on the problem

The README file contains:
- A description of the task
- The function definition and docstring
- Test failure information
- Instructions for running tests
- Information about test budget

This allows human evaluators to work in the same environment with the same constraints as the AI models, enabling fair comparisons.

Example command:
```sh
python -m runners.eval --model human --data data_file --n_problems 1 --output results/human_eval_results.jsonl
```

After human evaluators complete their work, you'll need to manually record their solutions, test results, and performance metrics to compare with AI models.

## Running your own agent

To test your own agent instead of our existing agent, you can overwrite the `CodeAgent` in `lib/agents.py`, respecting the input/output format, and then run it on the eval data you have using the provided commands. You can create a PR We may improve this to make it easier to port new agents in the future.

## License

Breakpoint is licensed under the MIT License. See [LICENSE](LICENSE) for details.
